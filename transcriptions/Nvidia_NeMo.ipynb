{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11HjdygAgw9Z2c5-m6xru2C5CjWbtkMik","timestamp":1673620932389}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"U90jLcxCCMrN"},"outputs":[],"source":["# Install Nvidia NeMo toolkit and its dependencies, including all optional components.\n","!pip install nemo_toolkit['all']\n","\n","# Import NeMo's core package.\n","import nemo\n","\n","# Import NeMo's ASR collection, which includes complete ASR models and building blocks.\n","import nemo.collections.asr as nemo_asr"]},{"cell_type":"code","source":["# Import other necessary libraries for data manipulation and audio file management.\n","import numpy as np\n","import pandas as pd\n","import librosa"],"metadata":{"id":"Jdd72LoUH3uk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Data"],"metadata":{"id":"Iku3WSS2t_-j"}},{"cell_type":"code","source":["# Mount the drive that contains wav files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-45t0FTG4d5","executionInfo":{"status":"ok","timestamp":1677761213523,"user_tz":-60,"elapsed":25498,"user":{"displayName":"Camille Landesvatter","userId":"08465821237712601560"}},"outputId":"eeb7b892-232b-4e14-a956-cfc9af81c44e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Find all file paths containing WAV files using librosa's utility function.\n","# Note: Files with a size less than 100KB (~ <2 seconds) were excluded in a previous step,\n","# as librosa.load cannot resample them to 16000Hz.\n","\n","\n","# Define the directory path where the WAV files are located.\n","files = librosa.util.find_files('path-containing-wav-files', ext='wav', recurse=False)\n","\n","# Convert the list of file paths to a NumPy array.\n","files = np.asarray(files)\n","\n","#len(files)\n","#files"],"metadata":{"id":"ZeBfTXaGHcC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loop over the WAV files and determine their sample rates using librosa.get_samplerate.\n","\n","for wav_file in files:\n","    sr = librosa.get_samplerate(wav_file)\n","    print(f'{wav_file}: {sr} Hz')"],"metadata":{"id":"Q1T5VET9UFdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In the same directory, resample and write all files to 16000hz\n","\n","from scipy.io import wavfile\n","\n","# Set the new sample rate\n","new_sample_rate = 16000\n","\n","# Loop over all files in the list\n","for filepath in files:\n","    # Load the audio data and current sample rate\n","    y, sr = librosa.load(filepath, sr=None)\n","\n","    # Resample the audio to the new sample rate\n","    y_resampled = librosa.resample(y, orig_sr=sr, target_sr=new_sample_rate)\n","\n","    # Write the resampled audio to a new WAV file\n","    wavfile.write(filepath, new_sample_rate, y_resampled)"],"metadata":{"id":"0F-Gnns8ZQsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the resampled files\n","\n","files_split1 = librosa.util.find_files('path-containing-resampled-wav-files', ext='wav')\n","files = np.asarray(files_split1)\n","\n","#len(files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UsYTDmvcr2Y8","executionInfo":{"status":"ok","timestamp":1677682988497,"user_tz":-60,"elapsed":1187,"user":{"displayName":"Camille Landesvatter","userId":"08465821237712601560"}},"outputId":"75189be5-be96-4f9c-b782-b7b6918ac2c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2993"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[" # Loop over the WAV files and get their sample rates\n","librosa.get_samplerate(files[1])\n","\n","#16000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agAGZTB_r6lJ","executionInfo":{"status":"ok","timestamp":1677682999807,"user_tz":-60,"elapsed":1061,"user":{"displayName":"Camille Landesvatter","userId":"08465821237712601560"}},"outputId":"c57d4d3a-88d6-4689-c13e-15f1e8af876a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16000"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["## Import Model"],"metadata":{"id":"tuvrsbDvuEy6"}},{"cell_type":"code","source":["# Specify the pretrained ASR (Automatic Speech Recognition) model to be used.\n","asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"stt_en_conformer_transducer_xxlarge\")"],"metadata":{"id":"b0rcWrLHu_QL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transcription"],"metadata":{"id":"iITTpJ-VuHrJ"}},{"cell_type":"code","source":["# Use \"%%capture\" to capture and suppress the output of the following cell.\n","%%capture\n","\n","# Initialize an empty list 'transcriptions_single' to store single transcriptions.\n","transcriptions = []\n","\n","# Iterate through the list of transcriptions and extract the second element (transcribed text) from each.\n","for i in files:\n","  x = asr_model.transcribe([i])\n","  transcriptions.append(x)"],"metadata":{"id":"5uPFEWodJT-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#len(transcriptions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhmI1KBa9_EM","executionInfo":{"status":"ok","timestamp":1677685395446,"user_tz":-60,"elapsed":408,"user":{"displayName":"Camille Landesvatter","userId":"08465821237712601560"}},"outputId":"9f97643a-fd1f-4faa-eafd-c074629ee7ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2993"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Initialize an empty list 'transcriptions_single' to store single transcriptions.\n","transcriptions_single=[]\n","#type(transcriptions_single)\n","\n","# Loop through the list of transcriptions and extract the second element (transcribed text) from each.\n","for i in transcriptions:\n","  transcriptions_single.append(i[1])"],"metadata":{"id":"23mqe00ULZet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge the transcribed text ('strings') with a 'data' DataFrame by adding a new column named 'transcription'.\n","data=[]\n","data[\"transcription\"]=transcriptions_single\n","data\n","\n","# To access the transcribed text for a specific sample, you can use:\n","#transcription[0][\"text\"]"],"metadata":{"id":"8Q0ckh0ywuOM"},"execution_count":null,"outputs":[]}]}